{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # China → US Spillover Analysis (Jupyter Notebook)\n",
    "# Simple, well-commented notebook that tests whether Chinese indices help predict US index returns.\n",
    "# Assumes your CSVs already have columns: Date, Close, Idxrtn (so no renaming required).\n",
    "\n",
    "\n",
    "### Quick overview\n",
    "# Steps:\n",
    "# 1. Load weekly data (assume weekly or resample to weekly)\n",
    "# 2. Compute weekly log returns\n",
    "# 3. Method A: VAR + Granger causality (lead-lag testing)\n",
    "# 4. Method B: Diebold-Yilmaz spillover index (FEVD-based)\n",
    "# 5. Method C: Volatility spillover via GARCH + rolling correlation of standardized residuals\n",
    "# 6. Method D: ML-approach: XGBoost\n",
    "# 7. Method E: ML-approach: small feed-forward neural net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas numpy matplotlib seaborn statsmodels arch xgboost tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "from arch import arch_model\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "us_path = 'us_index.csv'\n",
    "cn_path = 'cn_index.csv'\n",
    "\n",
    "\n",
    "us = pd.read_csv(us_path, parse_dates=['Date']).sort_values('Date')\n",
    "cn = pd.read_csv(cn_path, parse_dates=['Date']).sort_values('Date')\n",
    "\n",
    "\n",
    "# Resample both series to weekly (Friday close). If your data is already weekly, this will simply keep it.\n",
    "us_w = us.set_index('Date')['Close'].resample('W-FRI').last().ffill()\n",
    "cn_w = cn.set_index('Date')['Close'].resample('W-FRI').last().ffill()\n",
    "\n",
    "\n",
    "# Build DataFrame with log returns\n",
    "data = pd.DataFrame({'US_Close': us_w, 'CN_Close': cn_w}).dropna()\n",
    "# Log returns: simple and commonly used for VAR / GARCH\n",
    "data['US_Ret'] = np.log(data['US_Close']).diff()\n",
    "data['CN_Ret'] = np.log(data['CN_Close']).diff()\n",
    "# drop the first NA after differencing\n",
    "data = data.dropna().reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "\n",
    "print('Sample rows:')\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(data['Date'], data['US_Ret'], label='US weekly log return')\n",
    "plt.plot(data['Date'], data['CN_Ret'], label='CN weekly log return')\n",
    "plt.legend()\n",
    "plt.title('Weekly log returns (US vs China)')\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# Prepare data and fit VAR\n",
    "var_data = data[['US_Ret', 'CN_Ret']].set_index(data['Date'])\n",
    "model = VAR(var_data)\n",
    "res = model.fit(4)\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Granger Causality\n",
    "\n",
    "Idea (simple): fit a VAR on the two return series and run Granger causality tests.\n",
    "If lagged CN returns help predict US returns (statistically), that is evidence of return spillover from CN to US.\n",
    "\n",
    "\n",
    "Notes on interpretation:\n",
    "- Granger causality is not proof of economic causation. It tests whether past values of one series contain predictive information about another.\n",
    "- Use several lags. Here we use 4 weekly lags (~1 month).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and fit VAR\n",
    "var_data = data[['US_Ret', 'CN_Ret']].set_index(data['Date'])\n",
    "model = VAR(var_data)\n",
    "res = model.fit(4)\n",
    "print(res.summary())\n",
    "\n",
    "# The function expects an array where column 0 is the dependent variable\n",
    "# and column 1 is the potential cause. We test both directions.\n",
    "print('Granger test: does CN cause US? (null: no causality)')\n",
    "grangercausalitytests(var_data[['US_Ret','CN_Ret']], maxlag=4, verbose=True)\n",
    "\n",
    "\n",
    "print('Granger test: does US cause CN? (null: no causality)')\n",
    "grangercausalitytests(var_data[['CN_Ret','US_Ret']], maxlag=4, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diebold-Yilmaz spillover index\n",
    "Idea: use the VAR's forecast error variance decomposition (FEVD). The fraction of US forecast error variance\n",
    "explained by shocks to CN (the off-diagonal element) is a direct measure of spillover from CN to US.\n",
    "\n",
    "\n",
    "This is a simplified one-step FEVD approach. Robust research implementations use generalized FEVD and\n",
    "bootstrap inference. This version is for learning and quick checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10 # FEVD horizon in periods (weeks)\n",
    "fevd = res.fevd(h)\n",
    "# fevd.decomp: shape (h, neqs, neqs) — take the last horizon\n",
    "try:\n",
    "decomp = fevd.decomp[-1]\n",
    "except Exception:\n",
    "# statsmodels versions differ; fall back to direct attribute\n",
    "decomp = fevd._decomp[-1]\n",
    "\n",
    "\n",
    "# normalize rows to sum to 1 (so we interpret as shares)\n",
    "row_sums = decomp.sum(axis=1, keepdims=True)\n",
    "shares = decomp / row_sums\n",
    "\n",
    "\n",
    "# total spillover index: mean of off-diagonal shares\n",
    "n = shares.shape[0]\n",
    "off_diag = shares.sum() - np.trace(shares)\n",
    "total_spill = off_diag / n\n",
    "print(f'Total spillover index (h={h}): {total_spill:.4f}')\n",
    "\n",
    "\n",
    "spill_df = pd.DataFrame(shares, index=['US_recipient','CN_recipient'], columns=['US_source','CN_source'])\n",
    "print('FEVD share matrix (rows = recipient, cols = source):')\n",
    "print(spill_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Method C: Volatility spillover (GARCH + rolling correlation of standardized residuals)\n",
    "Idea: Fit univariate GARCH(1,1) to each return series to model conditional volatility.\n",
    "Then compute standardized residuals (residual / cond. vol). If those standardized residuals show\n",
    "time-varying correlation (e.g., rising after shocks), that is suggestive of volatility spillover / contagion.\n",
    "\n",
    "\n",
    "This is a clear, easy-to-implement proxy. For full multivariate modeling use DCC-GARCH or BEKK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit GARCH(1,1) to each returns series (scale returns because small numbers can cause numerical issues)\n",
    "us_garch = arch_model(data['US_Ret']*100, p=1, q=1).fit(disp='off')\n",
    "cn_garch = arch_model(data['CN_Ret']*100, p=1, q=1).fit(disp='off')\n",
    "\n",
    "\n",
    "# standardized residuals\n",
    "us_std = us_garch.resid / us_garch.conditional_volatility\n",
    "cn_std = cn_garch.resid / cn_garch.conditional_volatility\n",
    "\n",
    "\n",
    "# Rolling correlation of standardized residuals (window=52 weeks ~ 1 year)\n",
    "roll_corr = us_std.rolling(window=52, min_periods=20).corr(cn_std)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "# roll_corr aligns with the index of the original series (same length)\n",
    "plt.plot(data['Date'], roll_corr)\n",
    "plt.title('Rolling correlation of standardized residuals (window=52)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(f'Average rolling correlation: {roll_corr.mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "ML: Predict US returns using lagged China returns (XGBoost and small NN)\n",
    "Build simple lag features and evaluate using time-series cross-validation to avoid look-ahead bias.\n",
    "\n",
    "Rules used here:\n",
    "- Use lagged CN returns (1..4) and optionally lagged US returns for autoregressive info\n",
    "- Evaluate with TimeSeriesSplit, reporting RMSE and R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lags\n",
    "\n",
    "def make_lags(df, lags=4):\n",
    "    # df must have columns Date, US_Ret, CN_Ret\n",
    "    tmp = df.copy().set_index('Date')\n",
    "    for i in range(1, lags+1):\n",
    "    tmp[f'CN_lag{i}'] = tmp['CN_Ret'].shift(i)\n",
    "    tmp[f'US_lag{i}'] = tmp['US_Ret'].shift(i)\n",
    "    tmp = tmp.dropna().reset_index()\n",
    "    return tmp\n",
    "\n",
    "\n",
    "lags = 4\n",
    "ml_df = make_lags(data, lags=lags)\n",
    "features = [c for c in ml_df.columns if 'lag' in c]\n",
    "X = ml_df[features].values\n",
    "y = ml_df['US_Ret'].values\n",
    "\n",
    "\n",
    "print('ML sample size:', X.shape)\n",
    "print('Features:', features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost\n",
    "Simple regressor with default settings. TimeSeriesSplit prevents using future information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "preds_xgb = np.zeros_like(y)\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "X_train, X_test = X[train_idx], X[test_idx]\n",
    "y_train, y_test = y[train_idx], y[test_idx]\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "preds_xgb[test_idx] = model.predict(X_test)\n",
    "\n",
    "\n",
    "print('XGBoost RMSE:', np.sqrt(mean_squared_error(y, preds_xgb)))\n",
    "print('XGBoost R2:', r2_score(y, preds_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network\n",
    "Small fully connected network. We scale features and keep architecture tiny to avoid overfitting on small samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "preds_nn = np.zeros_like(y)\n",
    "\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X_scaled):\n",
    "    X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    nn = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "nn.compile(optimizer='adam', loss='mse')\n",
    "es = EarlyStopping(patience=10, restore_best_weights=True, verbose=0)\n",
    "nn.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0, callbacks=[es])\n",
    "preds_nn[test_idx] = nn.predict(X_test).flatten()\n",
    "\n",
    "\n",
    "print('NN RMSE:', np.sqrt(mean_squared_error(y, preds_nn)))\n",
    "print('NN R2:', r2_score(y, preds_nn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ML methods evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ML predictions vs. actual\n",
    "\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(ml_df['Date'], y, label='Actual US_Ret', linewidth=1)\n",
    "plt.plot(ml_df['Date'], preds_xgb, label='XGBoost', alpha=0.8)\n",
    "plt.plot(ml_df['Date'], preds_nn, label='Neural Net', alpha=0.8)\n",
    "plt.legend()\n",
    "plt.title('ML predictions vs actual US weekly returns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
